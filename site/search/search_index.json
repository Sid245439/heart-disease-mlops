{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Heart Disease MLOps \u00b6 Project documentation. Pages \u00b6 EDA Modeling Experiment Tracking Assets \u00b6 Static assets are under: doc/images/ doc/_static/","title":"Home"},{"location":"#heart-disease-mlops","text":"Project documentation.","title":"Heart Disease MLOps"},{"location":"#pages","text":"EDA Modeling Experiment Tracking","title":"Pages"},{"location":"#assets","text":"Static assets are under: doc/images/ doc/_static/","title":"Assets"},{"location":"eda/","text":"Exploratory Data Analysis (EDA) \u2014 Heart Disease UCI \u00b6 This document captures the complete EDA process used in this project. Primary EDA source: Notebook: exploration/eda.ipynb Generated figures: doc/images/ 1. Objective and Approach \u00b6 Goal: Understand the Heart Disease UCI dataset\u2019s structure, target definition, missingness, feature distributions, and relationships so we can design a robust preprocessing + modeling pipeline. Key EDA outputs (as required): Target/class balance visualization Feature distribution plots (categorical and numerical) Correlation heatmap Basic statistical association check for categorical features 2. Dataset Acquisition \u00b6 Two equivalent ways are supported in this repo: Reproducible download script (recommended for pipelines): Script: download_data.py Output: data/raw/heart_disease_raw.csv Run: python download_data.py This script downloads the data from the url https://archive.ics.uci.edu/static/public/45/data.csv . This is the 45th version of the data. Direct CSV read (used inside the EDA notebook): The notebook loads from: https://archive.ics.uci.edu/static/public/45/data.csv 3. Data Description (Features + Target) \u00b6 The dataset contains the commonly used 14 attributes (UCI Heart Disease): age : age (years) sex : 1=male, 0=female cp : chest pain type (1\u20134) trestbps : resting blood pressure (mm Hg) chol : serum cholesterol (mg/dl) fbs : fasting blood sugar > 120 mg/dl (1=true, 0=false) restecg : resting ECG results (0\u20132) thalach : max heart rate achieved exang : exercise induced angina (1=yes, 0=no) oldpeak : ST depression induced by exercise relative to rest slope : slope of peak exercise ST segment (1\u20133) ca : number of major vessels (0\u20133) thal : thalassemia indicator (commonly 3, 6, 7) num : original diagnosis label (0\u20134) Target definition used in this project \u00b6 The original num is multi-valued ( 0,1,2,3,4 ). To make this a binary classification problem: target = 0 if num == 0 (absence of disease) target = 1 if num != 0 (presence of disease) After deriving target , the notebook drops num to avoid leakage and ambiguity. 4. Basic Data Quality Checks \u00b6 The notebook performs initial inspection via: df.info() for dtypes and non-null counts df.describe() for numeric summary statistics Missing values \u00b6 During inspection, two columns stand out as having missing values: ca thal These are treated as categorical in analysis (even though they are encoded numerically) and are typically handled via mode/most-frequent imputation during preprocessing. Note: for the Chi-square test in the notebook, missing rows are excluded using dropna() to keep the statistical test well-defined. 5. Class Balance (Target Distribution) \u00b6 To validate the problem setup and understand potential imbalance, the notebook plots a countplot of target . Interpretation (as observed in the notebook): The dataset is not extremely imbalanced, but class proportions should still be verified before choosing metrics and thresholds. 6. Feature-Type Grouping for EDA \u00b6 For analysis, features are grouped into categorical vs numerical: Categorical (discrete, encoded) columns: sex, cp, fbs, restecg, exang, slope, ca, thal Numerical (continuous/ordinal) columns: age, trestbps, chol, thalach, oldpeak This grouping drives the choice of plots and informs later preprocessing decisions (encoding and scaling). 7. Univariate Analysis (Distributions) \u00b6 7.1 Categorical feature distributions by target \u00b6 The notebook generates countplots for each categorical feature with hue=\"target\" to compare category frequencies across the two classes. Typical EDA takeaways from these plots: Some categorical features show visibly different distributions between target=0 and target=1 , which suggests predictive signal. Some categories are dominant within a feature (rare categories exist), which is important for encoding choices and potential regularization. 7.2 Numerical feature distributions by target \u00b6 The notebook uses histogram + KDE overlays ( sns.histplot(..., kde=True) ) split by target. Notebook observation captured in comments: Most numerical variables appear roughly bell-shaped except oldpeak , which is more skewed. Implication for modeling (preprocessing decision driver): Standardization is generally suitable for most continuous features. Skewed features can sometimes benefit from alternative scaling (e.g., MinMax/robust scaling) or transformations, to be validated during model development. 8. Correlation Analysis \u00b6 To identify linear relationships and potential multicollinearity, the notebook computes a correlation matrix and visualizes it using a heatmap (values shown as percentages). Notebook conclusion: No pair of features exhibits extremely high correlation that would force removal purely due to multicollinearity concerns. Therefore, all features are retained for downstream model training (feature selection is deferred to modeling/validation). 9. Statistical Association for Categorical Features (Chi-square Test) \u00b6 To quantify whether categorical features are associated with the target, the notebook runs a Chi-square test ( sklearn.feature_selection.chi2 ) on the categorical subset (after dropping missing rows). Result interpretation recorded in the notebook: Most categorical variables show statistically significant association with the target (p-value < 0.05). fbs is not significant in isolation (very high p-value), but is still retained because: features can contribute jointly even if marginal association is weak final decision should be made using cross-validated model performance 10. Reproducibility: How to Regenerate EDA Outputs \u00b6 From a clean environment: Run the notebook end-to-end: Open: exploration/eda.ipynb Run all cells Figures are saved to: doc/images/ Optional: a lightweight script also exists for quick checks: src/eda.py prints basic info and writes plots into logs/ . 11. EDA Artifacts (for Report / Submission) \u00b6 The notebook saves the following figures (submission-ready): doc/images/heart_disease_target_distribution.png doc/images/heart_disease_categorical_distribution.png doc/images/heart_disease_numerical_distribution.png doc/images/heart_disease_correlation_matrix.png","title":"EDA"},{"location":"eda/#exploratory-data-analysis-eda-heart-disease-uci","text":"This document captures the complete EDA process used in this project. Primary EDA source: Notebook: exploration/eda.ipynb Generated figures: doc/images/","title":"Exploratory Data Analysis (EDA) \u2014 Heart Disease UCI"},{"location":"eda/#1-objective-and-approach","text":"Goal: Understand the Heart Disease UCI dataset\u2019s structure, target definition, missingness, feature distributions, and relationships so we can design a robust preprocessing + modeling pipeline. Key EDA outputs (as required): Target/class balance visualization Feature distribution plots (categorical and numerical) Correlation heatmap Basic statistical association check for categorical features","title":"1. Objective and Approach"},{"location":"eda/#2-dataset-acquisition","text":"Two equivalent ways are supported in this repo: Reproducible download script (recommended for pipelines): Script: download_data.py Output: data/raw/heart_disease_raw.csv Run: python download_data.py This script downloads the data from the url https://archive.ics.uci.edu/static/public/45/data.csv . This is the 45th version of the data. Direct CSV read (used inside the EDA notebook): The notebook loads from: https://archive.ics.uci.edu/static/public/45/data.csv","title":"2. Dataset Acquisition"},{"location":"eda/#3-data-description-features-target","text":"The dataset contains the commonly used 14 attributes (UCI Heart Disease): age : age (years) sex : 1=male, 0=female cp : chest pain type (1\u20134) trestbps : resting blood pressure (mm Hg) chol : serum cholesterol (mg/dl) fbs : fasting blood sugar > 120 mg/dl (1=true, 0=false) restecg : resting ECG results (0\u20132) thalach : max heart rate achieved exang : exercise induced angina (1=yes, 0=no) oldpeak : ST depression induced by exercise relative to rest slope : slope of peak exercise ST segment (1\u20133) ca : number of major vessels (0\u20133) thal : thalassemia indicator (commonly 3, 6, 7) num : original diagnosis label (0\u20134)","title":"3. Data Description (Features + Target)"},{"location":"eda/#target-definition-used-in-this-project","text":"The original num is multi-valued ( 0,1,2,3,4 ). To make this a binary classification problem: target = 0 if num == 0 (absence of disease) target = 1 if num != 0 (presence of disease) After deriving target , the notebook drops num to avoid leakage and ambiguity.","title":"Target definition used in this project"},{"location":"eda/#4-basic-data-quality-checks","text":"The notebook performs initial inspection via: df.info() for dtypes and non-null counts df.describe() for numeric summary statistics","title":"4. Basic Data Quality Checks"},{"location":"eda/#missing-values","text":"During inspection, two columns stand out as having missing values: ca thal These are treated as categorical in analysis (even though they are encoded numerically) and are typically handled via mode/most-frequent imputation during preprocessing. Note: for the Chi-square test in the notebook, missing rows are excluded using dropna() to keep the statistical test well-defined.","title":"Missing values"},{"location":"eda/#5-class-balance-target-distribution","text":"To validate the problem setup and understand potential imbalance, the notebook plots a countplot of target . Interpretation (as observed in the notebook): The dataset is not extremely imbalanced, but class proportions should still be verified before choosing metrics and thresholds.","title":"5. Class Balance (Target Distribution)"},{"location":"eda/#6-feature-type-grouping-for-eda","text":"For analysis, features are grouped into categorical vs numerical: Categorical (discrete, encoded) columns: sex, cp, fbs, restecg, exang, slope, ca, thal Numerical (continuous/ordinal) columns: age, trestbps, chol, thalach, oldpeak This grouping drives the choice of plots and informs later preprocessing decisions (encoding and scaling).","title":"6. Feature-Type Grouping for EDA"},{"location":"eda/#7-univariate-analysis-distributions","text":"","title":"7. Univariate Analysis (Distributions)"},{"location":"eda/#71-categorical-feature-distributions-by-target","text":"The notebook generates countplots for each categorical feature with hue=\"target\" to compare category frequencies across the two classes. Typical EDA takeaways from these plots: Some categorical features show visibly different distributions between target=0 and target=1 , which suggests predictive signal. Some categories are dominant within a feature (rare categories exist), which is important for encoding choices and potential regularization.","title":"7.1 Categorical feature distributions by target"},{"location":"eda/#72-numerical-feature-distributions-by-target","text":"The notebook uses histogram + KDE overlays ( sns.histplot(..., kde=True) ) split by target. Notebook observation captured in comments: Most numerical variables appear roughly bell-shaped except oldpeak , which is more skewed. Implication for modeling (preprocessing decision driver): Standardization is generally suitable for most continuous features. Skewed features can sometimes benefit from alternative scaling (e.g., MinMax/robust scaling) or transformations, to be validated during model development.","title":"7.2 Numerical feature distributions by target"},{"location":"eda/#8-correlation-analysis","text":"To identify linear relationships and potential multicollinearity, the notebook computes a correlation matrix and visualizes it using a heatmap (values shown as percentages). Notebook conclusion: No pair of features exhibits extremely high correlation that would force removal purely due to multicollinearity concerns. Therefore, all features are retained for downstream model training (feature selection is deferred to modeling/validation).","title":"8. Correlation Analysis"},{"location":"eda/#9-statistical-association-for-categorical-features-chi-square-test","text":"To quantify whether categorical features are associated with the target, the notebook runs a Chi-square test ( sklearn.feature_selection.chi2 ) on the categorical subset (after dropping missing rows). Result interpretation recorded in the notebook: Most categorical variables show statistically significant association with the target (p-value < 0.05). fbs is not significant in isolation (very high p-value), but is still retained because: features can contribute jointly even if marginal association is weak final decision should be made using cross-validated model performance","title":"9. Statistical Association for Categorical Features (Chi-square Test)"},{"location":"eda/#10-reproducibility-how-to-regenerate-eda-outputs","text":"From a clean environment: Run the notebook end-to-end: Open: exploration/eda.ipynb Run all cells Figures are saved to: doc/images/ Optional: a lightweight script also exists for quick checks: src/eda.py prints basic info and writes plots into logs/ .","title":"10. Reproducibility: How to Regenerate EDA Outputs"},{"location":"eda/#11-eda-artifacts-for-report-submission","text":"The notebook saves the following figures (submission-ready): doc/images/heart_disease_target_distribution.png doc/images/heart_disease_categorical_distribution.png doc/images/heart_disease_numerical_distribution.png doc/images/heart_disease_correlation_matrix.png","title":"11. EDA Artifacts (for Report / Submission)"},{"location":"experiment-tracking/","text":"","title":"Experiment Tracking"},{"location":"modeling/","text":"Feature Engineering & Model Development \u00b6 This document describes the complete modeling workflow for the Heart Disease UCI dataset: Feature engineering (encoding + scaling) Training at least two classification models Documented model selection/tuning Evaluation using appropriate metrics Reproducible pipeline implementation (scripted training) Primary sources: Notebook: exploration/modeling.ipynb (model experimentation and threshold analysis) Training pipeline: src/training.py (cross-validated tuning + MLflow logging) Preprocessing utilities: src/preprocessing.py 1. Problem Formulation \u00b6 Task: binary classification to predict heart disease risk. Target construction (binary): Original label num is multi-class (0\u20134). Project target uses: target = 1 if num != 0 , else 0 . This aligns with common clinical framing: any positive diagnosis is treated as \u201cdisease present\u201d. 2. Feature Set \u00b6 The modeling notebook explicitly selects a 13-feature input set plus the derived target: Categorical (discrete/encoded) features sex , cp , fbs , restecg , exang , slope , ca , thal Numeric features (standard-scaled) age , trestbps , chol , thalach Numeric feature (min-max scaled) oldpeak Rationale (from EDA + common practice): most continuous features are approximately well-behaved under standardization, while oldpeak is often skewed and can benefit from bounded scaling. 3. Data Splitting Strategy \u00b6 In exploration/modeling.ipynb , the dataset is split into: Train: 70% Validation: 15% Test: 15% Important details: stratify is used on the target to preserve class proportions across splits. random_state=42 ensures repeatability. Purpose: Validation set is used to tune decision threshold (see Section 6). Test set is held out for the final unbiased evaluation. 4. Preprocessing / Feature Engineering Pipeline \u00b6 The notebook uses a fully reproducible sklearn ColumnTransformer : 4.1 Numerical preprocessing \u00b6 Missing values imputed with median ( SimpleImputer(strategy=\"median\") ) Scaling: StandardScaler for age , trestbps , chol , thalach MinMaxScaler for oldpeak 4.2 Categorical preprocessing \u00b6 Missing values imputed with most frequent ( SimpleImputer(strategy=\"most_frequent\") ) One-hot encoding via OneHotEncoder(handle_unknown=\"ignore\") handle_unknown=\"ignore\" is important for robust inference when previously unseen categories appear. 4.3 Why a pipeline matters (MLOps relevance) \u00b6 This design prevents training/serving skew by ensuring that the exact same transformations used during training are applied at inference time. 5. Candidate Models (Notebook Experimentation) \u00b6 The notebook evaluates multiple classifiers under the same preprocessing pipeline: Logistic Regression ( class_weight=\"balanced\" ) Random Forest ( n_estimators=400 , class_weight=\"balanced\" ) Histogram-based Gradient Boosting ( HistGradientBoostingClassifier ) Support Vector Classifier ( SVC(class_weight=\"balanced\") ) Gaussian Naive Bayes ( GaussianNB ) XGBoost ( XGBClassifier , tuned with a conservative learning rate and many estimators) All candidates are trained as: Pipeline([(\"prep\", preprocessor), (\"clf\", model)]) This ensures a fair comparison with identical preprocessing. 6. Threshold Selection (Recall-Constrained Decision Rule) \u00b6 In healthcare-oriented risk prediction, false negatives can be more costly than false positives. The notebook therefore does not rely purely on the default 0.5 threshold. It defines a helper get_best_threshold(...) : Sweeps thresholds from 0.00 to 1.00 Chooses the highest threshold that achieves at least a target recall on validation Two recall constraints are used in the notebook: Candidate comparison: min_recall=0.99 Final Logistic Regression selection: min_recall=0.95 This produces a tuned operating point that prioritizes sensitivity (recall), then reports the resulting precision/accuracy/F1. Notes on probability handling: For models supporting predict_proba , probabilities are used. Otherwise, decision_function is used as a scoring proxy. 7. Evaluation Metrics \u00b6 The notebook reports standard classification metrics on validation and test: Accuracy Precision Recall F1-score Additionally, it visualizes a confusion matrix for the final selected model on the test set. Why these metrics: Accuracy can be misleading when classes are imbalanced. Precision/Recall/F1 provide a better picture of clinical screening tradeoffs. Threshold tuning is explicitly tied to a recall target. 8. Final Model Choice (Notebook) \u00b6 After comparing candidates, the notebook fits a final pipeline with: LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=42) and evaluates it using the validation-chosen threshold on the test set. Rationale for Logistic Regression (typical justifications consistent with the notebook setup): Strong baseline for tabular medical data Interpretable and stable Fast training and suitable for CI/CD retraining loops 9. Scripted, Reproducible Training (Project Pipeline) \u00b6 While the notebook focuses on experimentation and threshold analysis, the production-oriented training flow is implemented in src/training.py . 9.1 Models trained (assignment requirement) \u00b6 The pipeline trains two required models : Logistic Regression (with hyperparameter search over C ) Random Forest (with hyperparameter search over n_estimators , max_depth , min_samples_split ) 9.2 Cross-validation and tuning \u00b6 Both models are tuned with GridSearchCV using: cv=5 scoring=\"roc_auc\" This meets the assignment requirement to evaluate with cross-validation and a relevant metric (ROC-AUC). 9.3 Metrics collected \u00b6 The pipeline computes and logs (train + test where applicable): Accuracy Precision Recall F1 ROC-AUC 9.4 Model selection \u00b6 The pipeline selects the best model based on test ROC-AUC . 9.5 Artifacts saved \u00b6 The pipeline saves: models/best_model.pkl models/preprocessor.pkl These are the artifacts loaded by the serving API in app.py . 10. How to Reproduce Modeling End-to-End \u00b6 Option A \u2014 Notebook exploration \u00b6 Open and run: exploration/modeling.ipynb This reproduces the preprocessing pipeline, candidate comparison, threshold tuning, and confusion matrix visualization. Option B \u2014 Reproducible training pipeline (recommended) \u00b6 Download data: python download_data.py Train + log experiments: python -c \"from src.training import train_pipeline; train_pipeline('data/raw/heart_disease_raw.csv')\" View experiment tracking UI: mlflow ui --host 0.0.0.0 --port 5000 This will use the local mlruns/ directory and show runs for Logistic Regression and Random Forest (parameters, metrics, and model artifacts).","title":"Modeling"},{"location":"modeling/#feature-engineering-model-development","text":"This document describes the complete modeling workflow for the Heart Disease UCI dataset: Feature engineering (encoding + scaling) Training at least two classification models Documented model selection/tuning Evaluation using appropriate metrics Reproducible pipeline implementation (scripted training) Primary sources: Notebook: exploration/modeling.ipynb (model experimentation and threshold analysis) Training pipeline: src/training.py (cross-validated tuning + MLflow logging) Preprocessing utilities: src/preprocessing.py","title":"Feature Engineering &amp; Model Development"},{"location":"modeling/#1-problem-formulation","text":"Task: binary classification to predict heart disease risk. Target construction (binary): Original label num is multi-class (0\u20134). Project target uses: target = 1 if num != 0 , else 0 . This aligns with common clinical framing: any positive diagnosis is treated as \u201cdisease present\u201d.","title":"1. Problem Formulation"},{"location":"modeling/#2-feature-set","text":"The modeling notebook explicitly selects a 13-feature input set plus the derived target: Categorical (discrete/encoded) features sex , cp , fbs , restecg , exang , slope , ca , thal Numeric features (standard-scaled) age , trestbps , chol , thalach Numeric feature (min-max scaled) oldpeak Rationale (from EDA + common practice): most continuous features are approximately well-behaved under standardization, while oldpeak is often skewed and can benefit from bounded scaling.","title":"2. Feature Set"},{"location":"modeling/#3-data-splitting-strategy","text":"In exploration/modeling.ipynb , the dataset is split into: Train: 70% Validation: 15% Test: 15% Important details: stratify is used on the target to preserve class proportions across splits. random_state=42 ensures repeatability. Purpose: Validation set is used to tune decision threshold (see Section 6). Test set is held out for the final unbiased evaluation.","title":"3. Data Splitting Strategy"},{"location":"modeling/#4-preprocessing-feature-engineering-pipeline","text":"The notebook uses a fully reproducible sklearn ColumnTransformer :","title":"4. Preprocessing / Feature Engineering Pipeline"},{"location":"modeling/#41-numerical-preprocessing","text":"Missing values imputed with median ( SimpleImputer(strategy=\"median\") ) Scaling: StandardScaler for age , trestbps , chol , thalach MinMaxScaler for oldpeak","title":"4.1 Numerical preprocessing"},{"location":"modeling/#42-categorical-preprocessing","text":"Missing values imputed with most frequent ( SimpleImputer(strategy=\"most_frequent\") ) One-hot encoding via OneHotEncoder(handle_unknown=\"ignore\") handle_unknown=\"ignore\" is important for robust inference when previously unseen categories appear.","title":"4.2 Categorical preprocessing"},{"location":"modeling/#43-why-a-pipeline-matters-mlops-relevance","text":"This design prevents training/serving skew by ensuring that the exact same transformations used during training are applied at inference time.","title":"4.3 Why a pipeline matters (MLOps relevance)"},{"location":"modeling/#5-candidate-models-notebook-experimentation","text":"The notebook evaluates multiple classifiers under the same preprocessing pipeline: Logistic Regression ( class_weight=\"balanced\" ) Random Forest ( n_estimators=400 , class_weight=\"balanced\" ) Histogram-based Gradient Boosting ( HistGradientBoostingClassifier ) Support Vector Classifier ( SVC(class_weight=\"balanced\") ) Gaussian Naive Bayes ( GaussianNB ) XGBoost ( XGBClassifier , tuned with a conservative learning rate and many estimators) All candidates are trained as: Pipeline([(\"prep\", preprocessor), (\"clf\", model)]) This ensures a fair comparison with identical preprocessing.","title":"5. Candidate Models (Notebook Experimentation)"},{"location":"modeling/#6-threshold-selection-recall-constrained-decision-rule","text":"In healthcare-oriented risk prediction, false negatives can be more costly than false positives. The notebook therefore does not rely purely on the default 0.5 threshold. It defines a helper get_best_threshold(...) : Sweeps thresholds from 0.00 to 1.00 Chooses the highest threshold that achieves at least a target recall on validation Two recall constraints are used in the notebook: Candidate comparison: min_recall=0.99 Final Logistic Regression selection: min_recall=0.95 This produces a tuned operating point that prioritizes sensitivity (recall), then reports the resulting precision/accuracy/F1. Notes on probability handling: For models supporting predict_proba , probabilities are used. Otherwise, decision_function is used as a scoring proxy.","title":"6. Threshold Selection (Recall-Constrained Decision Rule)"},{"location":"modeling/#7-evaluation-metrics","text":"The notebook reports standard classification metrics on validation and test: Accuracy Precision Recall F1-score Additionally, it visualizes a confusion matrix for the final selected model on the test set. Why these metrics: Accuracy can be misleading when classes are imbalanced. Precision/Recall/F1 provide a better picture of clinical screening tradeoffs. Threshold tuning is explicitly tied to a recall target.","title":"7. Evaluation Metrics"},{"location":"modeling/#8-final-model-choice-notebook","text":"After comparing candidates, the notebook fits a final pipeline with: LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=42) and evaluates it using the validation-chosen threshold on the test set. Rationale for Logistic Regression (typical justifications consistent with the notebook setup): Strong baseline for tabular medical data Interpretable and stable Fast training and suitable for CI/CD retraining loops","title":"8. Final Model Choice (Notebook)"},{"location":"modeling/#9-scripted-reproducible-training-project-pipeline","text":"While the notebook focuses on experimentation and threshold analysis, the production-oriented training flow is implemented in src/training.py .","title":"9. Scripted, Reproducible Training (Project Pipeline)"},{"location":"modeling/#91-models-trained-assignment-requirement","text":"The pipeline trains two required models : Logistic Regression (with hyperparameter search over C ) Random Forest (with hyperparameter search over n_estimators , max_depth , min_samples_split )","title":"9.1 Models trained (assignment requirement)"},{"location":"modeling/#92-cross-validation-and-tuning","text":"Both models are tuned with GridSearchCV using: cv=5 scoring=\"roc_auc\" This meets the assignment requirement to evaluate with cross-validation and a relevant metric (ROC-AUC).","title":"9.2 Cross-validation and tuning"},{"location":"modeling/#93-metrics-collected","text":"The pipeline computes and logs (train + test where applicable): Accuracy Precision Recall F1 ROC-AUC","title":"9.3 Metrics collected"},{"location":"modeling/#94-model-selection","text":"The pipeline selects the best model based on test ROC-AUC .","title":"9.4 Model selection"},{"location":"modeling/#95-artifacts-saved","text":"The pipeline saves: models/best_model.pkl models/preprocessor.pkl These are the artifacts loaded by the serving API in app.py .","title":"9.5 Artifacts saved"},{"location":"modeling/#10-how-to-reproduce-modeling-end-to-end","text":"","title":"10. How to Reproduce Modeling End-to-End"},{"location":"modeling/#option-a-notebook-exploration","text":"Open and run: exploration/modeling.ipynb This reproduces the preprocessing pipeline, candidate comparison, threshold tuning, and confusion matrix visualization.","title":"Option A \u2014 Notebook exploration"},{"location":"modeling/#option-b-reproducible-training-pipeline-recommended","text":"Download data: python download_data.py Train + log experiments: python -c \"from src.training import train_pipeline; train_pipeline('data/raw/heart_disease_raw.csv')\" View experiment tracking UI: mlflow ui --host 0.0.0.0 --port 5000 This will use the local mlruns/ directory and show runs for Logistic Regression and Random Forest (parameters, metrics, and model artifacts).","title":"Option B \u2014 Reproducible training pipeline (recommended)"}]}