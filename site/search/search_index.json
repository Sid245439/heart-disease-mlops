{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Heart Disease MLOps","text":"<p>Project documentation.</p>"},{"location":"index.html#pages","title":"Pages","text":"<ul> <li>EDA</li> <li>Modeling</li> <li>Experiment Tracking</li> </ul>"},{"location":"index.html#assets","title":"Assets","text":"<p>Static assets are under:</p> <ul> <li><code>doc/images/</code></li> <li><code>doc/_static/</code></li> </ul>"},{"location":"eda.html","title":"Exploratory Data Analysis (EDA) \u2014 Heart Disease UCI","text":"<p>This document captures the complete EDA process used in this project.</p> <p>Primary EDA source:</p> <ul> <li>Notebook: <code>exploration/eda.ipynb</code></li> <li>Generated figures: <code>doc/images/</code></li> </ul>"},{"location":"eda.html#1-objective-and-approach","title":"1. Objective and Approach","text":"<p>Goal: Understand the Heart Disease UCI dataset\u2019s structure, target definition, missingness, feature distributions, and relationships so we can design a robust preprocessing + modeling pipeline.</p> <p>Key EDA outputs (as required):</p> <ul> <li>Target/class balance visualization</li> <li>Feature distribution plots (categorical and numerical)</li> <li>Correlation heatmap</li> <li>Basic statistical association check for categorical features</li> </ul>"},{"location":"eda.html#2-dataset-acquisition","title":"2. Dataset Acquisition","text":"<p>Two equivalent ways are supported in this repo:</p> <ol> <li> <p>Reproducible download script (recommended for pipelines):</p> </li> <li> <p>Script: <code>download_data.py</code></p> </li> <li>Output: <code>data/raw/heart_disease_raw.csv</code></li> </ol> <p>Run:</p> <pre><code>python download_data.py\n</code></pre> <p>This script downloads the data from the url <code>https://archive.ics.uci.edu/static/public/45/data.csv</code>. This is the 45th version of the data.</p> <ol> <li>Direct CSV read (used inside the EDA notebook):</li> </ol> <p>The notebook loads from:</p> <ul> <li>https://archive.ics.uci.edu/static/public/45/data.csv</li> </ul>"},{"location":"eda.html#3-data-description-features-target","title":"3. Data Description (Features + Target)","text":"<p>The dataset contains the commonly used 14 attributes (UCI Heart Disease):</p> <ul> <li><code>age</code>: age (years)</li> <li><code>sex</code>: 1=male, 0=female</li> <li><code>cp</code>: chest pain type (1\u20134)</li> <li><code>trestbps</code>: resting blood pressure (mm Hg)</li> <li><code>chol</code>: serum cholesterol (mg/dl)</li> <li><code>fbs</code>: fasting blood sugar &gt; 120 mg/dl (1=true, 0=false)</li> <li><code>restecg</code>: resting ECG results (0\u20132)</li> <li><code>thalach</code>: max heart rate achieved</li> <li><code>exang</code>: exercise induced angina (1=yes, 0=no)</li> <li><code>oldpeak</code>: ST depression induced by exercise relative to rest</li> <li><code>slope</code>: slope of peak exercise ST segment (1\u20133)</li> <li><code>ca</code>: number of major vessels (0\u20133)</li> <li><code>thal</code>: thalassemia indicator (commonly 3, 6, 7)</li> <li><code>num</code>: original diagnosis label (0\u20134)</li> </ul>"},{"location":"eda.html#target-definition-used-in-this-project","title":"Target definition used in this project","text":"<p>The original <code>num</code> is multi-valued (<code>0,1,2,3,4</code>). To make this a binary classification problem:</p> <ul> <li><code>target = 0</code> if <code>num == 0</code> (absence of disease)</li> <li><code>target = 1</code> if <code>num != 0</code> (presence of disease)</li> </ul> <p>After deriving <code>target</code>, the notebook drops <code>num</code> to avoid leakage and ambiguity.</p>"},{"location":"eda.html#4-basic-data-quality-checks","title":"4. Basic Data Quality Checks","text":"<p>The notebook performs initial inspection via:</p> <ul> <li><code>df.info()</code> for dtypes and non-null counts</li> <li><code>df.describe()</code> for numeric summary statistics</li> </ul>"},{"location":"eda.html#missing-values","title":"Missing values","text":"<p>During inspection, two columns stand out as having missing values:</p> <ul> <li><code>ca</code></li> <li><code>thal</code></li> </ul> <p>These are treated as categorical in analysis (even though they are encoded numerically) and are typically handled via mode/most-frequent imputation during preprocessing.</p> <p>Note: for the Chi-square test in the notebook, missing rows are excluded using <code>dropna()</code> to keep the statistical test well-defined.</p>"},{"location":"eda.html#5-class-balance-target-distribution","title":"5. Class Balance (Target Distribution)","text":"<p>To validate the problem setup and understand potential imbalance, the notebook plots a countplot of <code>target</code>.</p> <p></p> <p>Interpretation (as observed in the notebook):</p> <ul> <li>The dataset is not extremely imbalanced, but class proportions should still be verified before choosing metrics and thresholds.</li> </ul>"},{"location":"eda.html#6-feature-type-grouping-for-eda","title":"6. Feature-Type Grouping for EDA","text":"<p>For analysis, features are grouped into categorical vs numerical:</p> <p>Categorical (discrete, encoded) columns:</p> <pre><code>sex, cp, fbs, restecg, exang, slope, ca, thal\n</code></pre> <p>Numerical (continuous/ordinal) columns:</p> <pre><code>age, trestbps, chol, thalach, oldpeak\n</code></pre> <p>This grouping drives the choice of plots and informs later preprocessing decisions (encoding and scaling).</p>"},{"location":"eda.html#7-univariate-analysis-distributions","title":"7. Univariate Analysis (Distributions)","text":""},{"location":"eda.html#71-categorical-feature-distributions-by-target","title":"7.1 Categorical feature distributions by target","text":"<p>The notebook generates countplots for each categorical feature with <code>hue=\"target\"</code> to compare category frequencies across the two classes.</p> <p></p> <p>Typical EDA takeaways from these plots:</p> <ul> <li>Some categorical features show visibly different distributions between <code>target=0</code> and <code>target=1</code>, which suggests predictive signal.</li> <li>Some categories are dominant within a feature (rare categories exist), which is important for encoding choices and potential regularization.</li> </ul>"},{"location":"eda.html#72-numerical-feature-distributions-by-target","title":"7.2 Numerical feature distributions by target","text":"<p>The notebook uses histogram + KDE overlays (<code>sns.histplot(..., kde=True)</code>) split by target.</p> <p></p> <p>Notebook observation captured in comments:</p> <ul> <li>Most numerical variables appear roughly bell-shaped except <code>oldpeak</code>, which is more skewed.</li> </ul> <p>Implication for modeling (preprocessing decision driver):</p> <ul> <li>Standardization is generally suitable for most continuous features.</li> <li>Skewed features can sometimes benefit from alternative scaling (e.g., MinMax/robust scaling) or transformations, to be validated during model development.</li> </ul>"},{"location":"eda.html#8-correlation-analysis","title":"8. Correlation Analysis","text":"<p>To identify linear relationships and potential multicollinearity, the notebook computes a correlation matrix and visualizes it using a heatmap (values shown as percentages).</p> <p></p> <p>Notebook conclusion:</p> <ul> <li>No pair of features exhibits extremely high correlation that would force removal purely due to multicollinearity concerns.</li> <li>Therefore, all features are retained for downstream model training (feature selection is deferred to modeling/validation).</li> </ul>"},{"location":"eda.html#9-statistical-association-for-categorical-features-chi-square-test","title":"9. Statistical Association for Categorical Features (Chi-square Test)","text":"<p>To quantify whether categorical features are associated with the target, the notebook runs a Chi-square test (<code>sklearn.feature_selection.chi2</code>) on the categorical subset (after dropping missing rows).</p> <p>Result interpretation recorded in the notebook:</p> <ul> <li>Most categorical variables show statistically significant association with the target (p-value &lt; 0.05).</li> <li><code>fbs</code> is not significant in isolation (very high p-value), but is still retained because:</li> <li>features can contribute jointly even if marginal association is weak</li> <li>final decision should be made using cross-validated model performance</li> </ul>"},{"location":"eda.html#10-reproducibility-how-to-regenerate-eda-outputs","title":"10. Reproducibility: How to Regenerate EDA Outputs","text":"<p>From a clean environment:</p> <ol> <li> <p>Run the notebook end-to-end:</p> </li> <li> <p>Open: <code>exploration/eda.ipynb</code></p> </li> <li>Run all cells</li> <li>Figures are saved to: <code>doc/images/</code></li> </ol> <p>Optional: a lightweight script also exists for quick checks:</p> <ul> <li><code>src/eda.py</code> prints basic info and writes plots into <code>logs/</code>.</li> </ul>"},{"location":"eda.html#11-eda-artifacts-for-report-submission","title":"11. EDA Artifacts (for Report / Submission)","text":"<p>The notebook saves the following figures (submission-ready):</p> <ul> <li><code>doc/images/heart_disease_target_distribution.png</code></li> <li><code>doc/images/heart_disease_categorical_distribution.png</code></li> <li><code>doc/images/heart_disease_numerical_distribution.png</code></li> <li><code>doc/images/heart_disease_correlation_matrix.png</code></li> </ul>"},{"location":"modeling.html","title":"Feature Engineering &amp; Model Development","text":"<p>This document describes the complete modeling workflow for the Heart Disease UCI dataset:</p> <ul> <li>Feature engineering (encoding + scaling)</li> <li>Training at least two classification models</li> <li>Documented model selection/tuning</li> <li>Evaluation using appropriate metrics</li> <li>Reproducible pipeline implementation (scripted training)</li> </ul> <p>Primary sources:</p> <ul> <li>Notebook: <code>exploration/modeling.ipynb</code> (model experimentation and threshold analysis)</li> <li>Training pipeline: <code>src/training.py</code> (cross-validated tuning + MLflow logging)</li> <li>Preprocessing utilities: <code>src/preprocessing.py</code></li> </ul>"},{"location":"modeling.html#1-problem-formulation","title":"1. Problem Formulation","text":"<p>Task: binary classification to predict heart disease risk.</p> <p>Target construction (binary):</p> <ul> <li>Original label <code>num</code> is multi-class (0\u20134).</li> <li>Project target uses: <code>target = 1</code> if <code>num != 0</code>, else <code>0</code>.</li> </ul> <p>This aligns with common clinical framing: any positive diagnosis is treated as \u201cdisease present\u201d.</p>"},{"location":"modeling.html#2-feature-set","title":"2. Feature Set","text":"<p>The modeling notebook explicitly selects a 13-feature input set plus the derived target:</p> <p>Categorical (discrete/encoded) features</p> <ul> <li><code>sex</code>, <code>cp</code>, <code>fbs</code>, <code>restecg</code>, <code>exang</code>, <code>slope</code>, <code>ca</code>, <code>thal</code></li> </ul> <p>Numeric features (standard-scaled)</p> <ul> <li><code>age</code>, <code>trestbps</code>, <code>chol</code>, <code>thalach</code></li> </ul> <p>Numeric feature (min-max scaled)</p> <ul> <li><code>oldpeak</code></li> </ul> <p>Rationale (from EDA + common practice): most continuous features are approximately well-behaved under standardization, while <code>oldpeak</code> is often skewed and can benefit from bounded scaling.</p>"},{"location":"modeling.html#3-data-splitting-strategy","title":"3. Data Splitting Strategy","text":"<p>In <code>exploration/modeling.ipynb</code>, the dataset is split into:</p> <ul> <li>Train: 70%</li> <li>Validation: 15%</li> <li>Test: 15%</li> </ul> <p>Important details:</p> <ul> <li><code>stratify</code> is used on the target to preserve class proportions across splits.</li> <li><code>random_state=42</code> ensures repeatability.</li> </ul> <p>Purpose:</p> <ul> <li>Validation set is used to tune decision threshold (see Section 6).</li> <li>Test set is held out for the final unbiased evaluation.</li> </ul>"},{"location":"modeling.html#4-preprocessing-feature-engineering-pipeline","title":"4. Preprocessing / Feature Engineering Pipeline","text":"<p>The notebook uses a fully reproducible <code>sklearn</code> <code>ColumnTransformer</code>:</p>"},{"location":"modeling.html#41-numerical-preprocessing","title":"4.1 Numerical preprocessing","text":"<ul> <li>Missing values imputed with median (<code>SimpleImputer(strategy=\"median\")</code>)</li> <li>Scaling:</li> <li>StandardScaler for <code>age</code>, <code>trestbps</code>, <code>chol</code>, <code>thalach</code></li> <li>MinMaxScaler for <code>oldpeak</code></li> </ul>"},{"location":"modeling.html#42-categorical-preprocessing","title":"4.2 Categorical preprocessing","text":"<ul> <li>Missing values imputed with most frequent (<code>SimpleImputer(strategy=\"most_frequent\")</code>)</li> <li>One-hot encoding via <code>OneHotEncoder(handle_unknown=\"ignore\")</code></li> <li><code>handle_unknown=\"ignore\"</code> is important for robust inference when previously unseen categories appear.</li> </ul>"},{"location":"modeling.html#43-why-a-pipeline-matters-mlops-relevance","title":"4.3 Why a pipeline matters (MLOps relevance)","text":"<p>This design prevents training/serving skew by ensuring that the exact same transformations used during training are applied at inference time.</p>"},{"location":"modeling.html#5-candidate-models-notebook-experimentation","title":"5. Candidate Models (Notebook Experimentation)","text":"<p>The notebook evaluates multiple classifiers under the same preprocessing pipeline:</p> <ul> <li>Logistic Regression (<code>class_weight=\"balanced\"</code>)</li> <li>Random Forest (<code>n_estimators=400</code>, <code>class_weight=\"balanced\"</code>)</li> <li>Histogram-based Gradient Boosting (<code>HistGradientBoostingClassifier</code>)</li> <li>Support Vector Classifier (<code>SVC(class_weight=\"balanced\")</code>)</li> <li>Gaussian Naive Bayes (<code>GaussianNB</code>)</li> <li>XGBoost (<code>XGBClassifier</code>, tuned with a conservative learning rate and many estimators)</li> </ul> <p>All candidates are trained as:</p> <ul> <li><code>Pipeline([(\"prep\", preprocessor), (\"clf\", model)])</code></li> </ul> <p>This ensures a fair comparison with identical preprocessing.</p>"},{"location":"modeling.html#6-threshold-selection-recall-constrained-decision-rule","title":"6. Threshold Selection (Recall-Constrained Decision Rule)","text":"<p>In healthcare-oriented risk prediction, false negatives can be more costly than false positives. The notebook therefore does not rely purely on the default 0.5 threshold.</p> <p>It defines a helper <code>get_best_threshold(...)</code>:</p> <ul> <li>Sweeps thresholds from 0.00 to 1.00</li> <li>Chooses the highest threshold that achieves at least a target recall on validation</li> </ul> <p>Two recall constraints are used in the notebook:</p> <ul> <li>Candidate comparison: <code>min_recall=0.99</code></li> <li>Final Logistic Regression selection: <code>min_recall=0.95</code></li> </ul> <p>This produces a tuned operating point that prioritizes sensitivity (recall), then reports the resulting precision/accuracy/F1.</p> <p>Notes on probability handling:</p> <ul> <li>For models supporting <code>predict_proba</code>, probabilities are used.</li> <li>Otherwise, <code>decision_function</code> is used as a scoring proxy.</li> </ul>"},{"location":"modeling.html#7-evaluation-metrics","title":"7. Evaluation Metrics","text":"<p>The notebook reports standard classification metrics on validation and test:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> </ul> <p>Additionally, it visualizes a confusion matrix for the final selected model on the test set.</p> <p></p> <p>Why these metrics:</p> <ul> <li>Accuracy can be misleading when classes are imbalanced.</li> <li>Precision/Recall/F1 provide a better picture of clinical screening tradeoffs.</li> <li>Threshold tuning is explicitly tied to a recall target.</li> </ul>"},{"location":"modeling.html#8-final-model-choice-notebook","title":"8. Final Model Choice (Notebook)","text":"<p>After comparing candidates, the notebook fits a final pipeline with:</p> <ul> <li><code>LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=42)</code></li> </ul> <p>and evaluates it using the validation-chosen threshold on the test set.</p> <p>Rationale for Logistic Regression (typical justifications consistent with the notebook setup):</p> <ul> <li>Strong baseline for tabular medical data</li> <li>Interpretable and stable</li> <li>Fast training and suitable for CI/CD retraining loops</li> </ul>"},{"location":"modeling.html#9-scripted-reproducible-training-project-pipeline","title":"9. Scripted, Reproducible Training (Project Pipeline)","text":"<p>While the notebook focuses on experimentation and threshold analysis, the production-oriented training flow is implemented in <code>src/training.py</code>.</p>"},{"location":"modeling.html#91-models-trained-assignment-requirement","title":"9.1 Models trained (assignment requirement)","text":"<p>The pipeline trains two required models:</p> <ul> <li>Logistic Regression (with hyperparameter search over <code>C</code>)</li> <li>Random Forest (with hyperparameter search over <code>n_estimators</code>, <code>max_depth</code>, <code>min_samples_split</code>)</li> </ul>"},{"location":"modeling.html#92-cross-validation-and-tuning","title":"9.2 Cross-validation and tuning","text":"<p>Both models are tuned with <code>GridSearchCV</code> using:</p> <ul> <li><code>cv=5</code></li> <li><code>scoring=\"roc_auc\"</code></li> </ul> <p>This meets the assignment requirement to evaluate with cross-validation and a relevant metric (ROC-AUC).</p>"},{"location":"modeling.html#93-metrics-collected","title":"9.3 Metrics collected","text":"<p>The pipeline computes and logs (train + test where applicable):</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1</li> <li>ROC-AUC</li> </ul>"},{"location":"modeling.html#94-model-selection","title":"9.4 Model selection","text":"<p>The pipeline selects the best model based on test ROC-AUC.</p>"},{"location":"modeling.html#95-artifacts-saved","title":"9.5 Artifacts saved","text":"<p>The pipeline saves:</p> <ul> <li><code>models/best_model.pkl</code></li> <li><code>models/preprocessor.pkl</code></li> </ul> <p>These are the artifacts loaded by the serving API in <code>app.py</code>.</p>"},{"location":"modeling.html#10-how-to-reproduce-modeling-end-to-end","title":"10. How to Reproduce Modeling End-to-End","text":""},{"location":"modeling.html#option-a-notebook-exploration","title":"Option A \u2014 Notebook exploration","text":"<p>Open and run:</p> <ul> <li><code>exploration/modeling.ipynb</code></li> </ul> <p>This reproduces the preprocessing pipeline, candidate comparison, threshold tuning, and confusion matrix visualization.</p>"},{"location":"modeling.html#option-b-reproducible-training-pipeline-recommended","title":"Option B \u2014 Reproducible training pipeline (recommended)","text":"<ol> <li>Download data:</li> </ol> <pre><code>python download_data.py\n</code></pre> <ol> <li>Train + log experiments:</li> </ol> <pre><code>python -c \"from src.training import train_pipeline; train_pipeline('data/raw/heart_disease_raw.csv')\"\n</code></pre> <ol> <li>View experiment tracking UI:</li> </ol> <pre><code>mlflow ui --host 0.0.0.0 --port 5000\n</code></pre> <p>This will use the local <code>mlruns/</code> directory and show runs for Logistic Regression and Random Forest (parameters, metrics, and model artifacts).</p>"}]}