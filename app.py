"""FastAPI Prediction Service with logging + Prometheus metrics."""

import logging
import os
import pickle  # noqa: S403 # Used for loading ML models generated by the training step no other use
import sys
import time
from collections.abc import AsyncIterator, Awaitable, Callable
from contextlib import asynccontextmanager
from pathlib import Path

import pandas as pd
from fastapi import FastAPI, HTTPException, Request, Response
from loguru import logger
from prometheus_client import CONTENT_TYPE_LATEST, Counter, Histogram, generate_latest
from pydantic import BaseModel

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)


class _InterceptHandler(logging.Handler):
    def emit(self, record: logging.LogRecord) -> None:  # noqa: PLR6301
        """Forward standard logging records to Loguru."""
        level: str | int
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        logger.opt(depth=6, exception=record.exc_info).log(level, record.getMessage())


def _configure_logging() -> None:
    log_format = "{time:YYYY-MM-DD HH:mm:ss} [{level}] {name} - {message}"
    logger.remove()
    logger.add(sys.stdout, level=LOG_LEVEL, format=log_format)
    logger.add(str(LOG_DIR / "api.log"), level=LOG_LEVEL, format=log_format)

    logging.basicConfig(handlers=[_InterceptHandler()], level=0, force=True)
    for name in ("uvicorn", "uvicorn.error", "uvicorn.access", "fastapi"):
        logging_logger = logging.getLogger(name)
        logging_logger.handlers = [_InterceptHandler()]
        logging_logger.propagate = False


_configure_logging()


# Prometheus metrics
REQUEST_COUNT = Counter(
    "heart_api_requests_total",
    "Total API requests",
    ["method", "endpoint", "http_status"],
)
REQUEST_LATENCY = Histogram(
    "heart_api_request_latency_seconds",
    "Request latency (s)",
    ["endpoint"],
)

ARTIFACTS = {}


class PatientData(BaseModel):
    """Patient data schema for heart disease prediction.

    Attributes
    ----------
    age : float
        Patient age in years
    sex : int
        Sex (1 = male, 0 = female)
    cp : int
        Chest pain type (0-3)
    trestbps : float
        Resting blood pressure (mm Hg)
    chol : float
        Serum cholesterol (mg/dl)
    fbs : int
        Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)
    restecg : int
        Resting electrocardiographic results (0-2)
    thalach : float
        Maximum heart rate achieved
    exang : int
        Exercise induced angina (1 = yes, 0 = no)
    oldpeak : float
        ST depression induced by exercise relative to rest
    slope : int
        Slope of the peak exercise ST segment (0-2)
    ca : int
        Number of major vessels colored by fluoroscopy (0-3)
    thal : int
        Thalassemia (0-3)

    """

    age: float
    sex: int
    cp: int
    trestbps: float
    chol: float
    fbs: int
    restecg: int
    thalach: float
    exang: int
    oldpeak: float
    slope: int
    ca: int
    thal: int


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncIterator[None]:  # noqa: ARG001, RUF029
    """Lifespan context manager to load models on startup."""
    # Load the models when the app starts

    with Path("models/best_model.pkl").open("rb") as f:  # noqa: ASYNC230
        ARTIFACTS["model"] = pickle.load(f)  # noqa: S301

    with Path("models/preprocessor.pkl").open("rb") as f:  # noqa: ASYNC230
        ARTIFACTS["preprocessor"] = pickle.load(f)  # noqa: S301
    yield

    # Cleanup up the ML artifacts on shutdown
    ARTIFACTS.clear()


app = FastAPI(title="Heart Disease Prediction API", lifespan=lifespan)


@app.middleware("http")
async def log_and_measure(
    request: Request,
    call_next: Callable[[Request], Awaitable[Response]],
) -> Response:
    """Log every request and track Prometheus metrics.

    Parameters
    ----------
    request : Request
        Incoming HTTP request
    call_next : Callable[[Request], Awaitable[Response]]
        Function to call the next middleware or endpoint

    Returns
    -------
    Response
        HTTP response from the endpoint

    """
    start_time = time.perf_counter()
    status_code = 500
    try:
        response = await call_next(request)
        status_code = response.status_code
        return response
    finally:
        duration = time.perf_counter() - start_time
        endpoint = request.url.path
        REQUEST_COUNT.labels(method=request.method, endpoint=endpoint, http_status=status_code).inc()
        REQUEST_LATENCY.labels(endpoint=endpoint).observe(duration)
        logger.info(
            "{} {} status={} duration_ms={:.2f}",
            request.method,
            endpoint,
            status_code,
            duration * 1000,
        )


@app.get("/health")
async def health() -> dict[str, object]:
    """Health check endpoint.

    Returns
    -------
    dict[str, object]
        Service status and whether the model is loaded in memory.

    """
    return {"status": "healthy", "model_loaded": "model" in ARTIFACTS}


@app.get("/metrics")
async def metrics() -> Response:
    """Prometheus scrape endpoint.

    Returns
    -------
    Response
        Prometheus metrics in text format.

    """
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.post("/predict")
async def predict(patient: PatientData) -> dict[str, object]:
    """Predict heart disease risk for a patient.

    Parameters
    ----------
    patient : PatientData
        Patient features used by the model for inference.

    Returns
    -------
    dict[str, object]
        A JSON-serializable dict containing:
        - prediction: int (model class prediction)
        - confidence: float (max predicted probability)
        - risk_level: str ("low" | "medium" | "high")

    Raises
    ------
    HTTPException
        If the model artifacts are not loaded, or if prediction fails due to invalid input.

    """
    if "model" not in ARTIFACTS or "preprocessor" not in ARTIFACTS:
        raise HTTPException(status_code=500, detail="Model not loaded")

    try:
        input_df = pd.DataFrame([patient.model_dump()])
        input_processed = ARTIFACTS["preprocessor"].transform(input_df)
        prediction = ARTIFACTS["model"].predict(input_processed)[0]
        confidence = max(ARTIFACTS["model"].predict_proba(input_processed)[0])
        risk_level = "low" if confidence < 0.6 else ("medium" if confidence < 0.8 else "high")  # noqa: PLR2004

        logger.info(
            "prediction={} risk={} confidence={:.3f} age={}",
            int(prediction),
            risk_level,
            confidence,
            patient.age,
        )

        return {
            "prediction": int(prediction),
            "confidence": float(confidence),
            "risk_level": risk_level,
        }
    except Exception as e:
        logger.exception("Prediction failed")
        raise HTTPException(400, str(e)) from e


@app.get("/")
async def root() -> dict[str, object]:
    """Root endpoint listing basic service info and available routes.

    Returns
    -------
    dict[str, object]
        Service name and a map of exposed endpoints to HTTP methods.

    """
    return {
        "service": "Heart Disease Prediction API",
        "endpoints": {"/health": "GET", "/predict": "POST", "/metrics": "GET"},
    }
